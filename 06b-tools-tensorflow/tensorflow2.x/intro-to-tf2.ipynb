{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data-x](https://raw.githubusercontent.com/afo/data-x-plaksha/master/imgsource/dx_logo.png)\n",
    "\n",
    "\n",
    "# Data-X: Introduction to TensorFlow 2.0, Tensorboard, and Keras\n",
    "\n",
    "**Author:** Alexander Fred-Ojala\n",
    "\n",
    "**Sources:** Francois Chollet, Sebastian Raschka, Aurélien Géron, etc.\n",
    "\n",
    "**Copright:** Feel free to do whatever you want with this code.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow:\n",
    "TensorFlow is the most popular and adopted free and open-source deep learning library. It was first developed and maintained by Google. It can be used for both research and production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TensorFlow benefits:**\n",
    "- Highly efficient\n",
    "- Cross-platform (works on IOS, Android, Unix, Windows, in the cloud, in the browser etc etc)\n",
    "- Calculates gradients automatically (this is truly useful for Neural Networks, where the analytical solution of gradients would be VERY tedious to derive).\n",
    "* Deep integration with the Keras library (Functional approach, as well as high-level wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install TensorFlow 2.0\n",
    "\n",
    "TensorFlow 2.x is a major change from TensorFlow 1.x (not backwards compatible, however you can use a tool to convert your TensorFlow 1.x code to 2.x).\n",
    "\n",
    "The new version is designed to be more pythonic. It's  easier to debug models, extract values during training (because of the need of sessions and graphs in TensorFlow 1.x). \n",
    "\n",
    "TensorFlow 2.x supports eager execution by default, so you don't need a session and to evaluate operations / tensors in order to extract values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.2.0rc2-cp36-cp36m-macosx_10_11_x86_64.whl (175.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 175.3 MB 74 kB/s s eta 0:00:01     |███████████████████████████▉    | 152.4 MB 49.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.28.1-cp36-cp36m-macosx_10_9_x86_64.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 24.6 MB/s eta 0:00:01     |████████████████▏               | 1.3 MB 24.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Downloading protobuf-3.11.3-cp36-cp36m-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 19.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /Users/deirdrequillen/opt/anaconda2/envs/py36/lib/python3.6/site-packages (from tensorflow) (1.18.1)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Using cached absl-py-0.9.0.tar.gz (104 kB)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 7.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.3.0,>=2.2.0rc0\n",
      "  Downloading tensorflow_estimator-2.2.0rc0-py2.py3-none-any.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 13.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-macosx_10_6_intel.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 17.2 MB/s eta 0:00:01     |███▊                            | 348 kB 17.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /Users/deirdrequillen/opt/anaconda2/envs/py36/lib/python3.6/site-packages (from tensorflow) (1.4.1)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.2.0-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 6.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.0\n",
      "  Using cached Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /Users/deirdrequillen/opt/anaconda2/envs/py36/lib/python3.6/site-packages (from tensorflow) (0.34.2)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.0-py3-none-any.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 17.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.12.0 in /Users/deirdrequillen/.local/lib/python3.6/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /Users/deirdrequillen/opt/anaconda2/envs/py36/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow) (46.0.0.post20200309)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.6.0.post2-py3-none-any.whl (775 kB)\n",
      "\u001b[K     |████████████████████████████████| 775 kB 41.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.1-py2.py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 6.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 20.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.13.1-py2.py3-none-any.whl (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 16.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/deirdrequillen/opt/anaconda2/envs/py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.25.8)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 9.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/deirdrequillen/opt/anaconda2/envs/py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2019.11.28)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.0.0-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<4.1,>=3.1.4\n",
      "  Using cached rsa-4.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Building wheels for collected packages: termcolor, absl-py, wrapt\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=f363350e6abb8548f0a0e000c154c898155ef65a205390d6dd6d1bd4d64f9c35\n",
      "  Stored in directory: /Users/deirdrequillen/Library/Caches/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=892cf5241a25913d58461e720798cc2fe00dbf620e7b2144afac5d1acba5d4ca\n",
      "  Stored in directory: /Users/deirdrequillen/Library/Caches/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-macosx_10_9_x86_64.whl size=32333 sha256=de22b7edffae40ae20b8baf35383ac61f7b64874ad6f4ec4bdd8fead926c442c\n",
      "  Stored in directory: /Users/deirdrequillen/Library/Caches/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
      "Successfully built termcolor absl-py wrapt\n",
      "Installing collected packages: termcolor, grpcio, astunparse, protobuf, gast, absl-py, google-pasta, tensorflow-estimator, h5py, opt-einsum, keras-preprocessing, wrapt, tensorboard-plugin-wit, markdown, idna, chardet, requests, werkzeug, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.0.0 chardet-3.0.4 gast-0.3.3 google-auth-1.13.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.28.1 h5py-2.10.0 idna-2.9 keras-preprocessing-1.1.0 markdown-3.2.1 oauthlib-3.1.0 opt-einsum-3.2.0 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 tensorboard-2.2.0 tensorboard-plugin-wit-1.6.0.post2 tensorflow-2.2.0rc2 tensorflow-estimator-2.2.0rc0 termcolor-1.1.0 werkzeug-1.0.1 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U --pre tensorflow\n",
    "# or for GPU version:\n",
    "# !pip install -U --pre tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonical way of importing TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# If this doesn't work TensorFlow is not installed correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 2.0 alpha\n",
    "At the time of the update of this notebook we are still in the early days of TensorFlow, and currently (March 19) the version 2.0.0-alpha0 is the latest one to be released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0-rc2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check tf version, oftentimes tensorflow is not backwards compatible\n",
    "tf.__version__\n",
    "\n",
    "# should be tensorflow 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to TensorFlow\n",
    "### Core components:\n",
    "\n",
    "#### 1. Tensor\n",
    "A Tensor in TensorFlow is an N-dimensional array (just like Numpys array object). Tensors are multilinear maps from vector spaces to real numbers. Scalars, vectors and matrices are all tensors. The Tensor represents units of data in TensorFlow.\n",
    "\n",
    "Numpy arrays or Pandas DataFrames sent to Tensorflow functions are automatically converted into TensorFlow tensors.\n",
    "\n",
    "#### 2. Operations / Ops\n",
    "TensorFlow operations or ops are units / edges / nodes of computation (e.g. matrix multiplication, addition, etc.)\n",
    "\n",
    "#### 3. Computation Graph\n",
    "The computational graph is is an optimized, compiled representation of the dataflow and the order of computations that are sent to an execution environment (for example during model training).\n",
    "\n",
    "TensforFlow 2.x supports eager execution, but when we build a model and then train it TensorFlow we can compile the model and optimize the executions as a computational graph object. This is done by decorating a function with `@tf.function`.\n",
    "\n",
    "This computational graph is then  sent to another instance / runtime environment (e.g. on a CPU or GPU) for execution. The results are sent back to us. This makes TensorFlow computations highly distributable and it also allows us to automatically evaluate all gradients in the computation nodes.\n",
    "\n",
    "![](imgs/tf_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 2.x supports eager execution by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard setup\n",
    "Tip2: Setup TensorBoard if you want to monitor and analyze computational graphs etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "t = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\") \n",
    "log_dir = \"tf_logs\"\n",
    "logd = \"/tmp/{}/r{}/\".format(log_dir, t)\n",
    "\n",
    "# Make directory if it doesn't exist\n",
    "\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "\n",
    "logdir = os.path.join(os.sep,home,logd)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TensorFlow tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 tf.constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants are initialized directly and eager execution let's us see the values without creating a session and running the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(2)\n",
    "b = tf.constant(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a # note the numpy value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .numpy() method will return the result as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eager evaluation of tensors\n",
    "a.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also perform operations on tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### or the same with universal functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.multiply(a,b).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[5, 6],\n",
       "       [7, 8]], dtype=int32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_matrix = tf.constant([[1,2], [3,4]])\n",
    "b_matrix = tf.constant([[5,6], [7,8]])\n",
    "b_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[19, 22],\n",
       "       [43, 50]], dtype=int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(a_matrix, b_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note, we cannot reassign values of constants (like is the case with Variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot assign contstants\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    a.assign(8)\n",
    "except:\n",
    "    print('Cannot assign contstants')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 tf.Variable\n",
    "\n",
    "Variables are mutable and can be updated and reassigned new values. Variables are usually weights and biases of a model that are optimized during training, they also indicate the degrees of freedom of the model (what model parameters that can change, thus making the model flexible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = tf.Variable(3.)\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reassign the value of a Variable\n",
    "var.assign(4)\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(3, 1) dtype=float64, numpy=\n",
       "array([[-0.7604148 ],\n",
       "       [ 0.88886952],\n",
       "       [ 0.41986008]])>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also create multi dim Variables.\n",
    "d = tf.Variable(np.random.randn(3).reshape(3,1)) #reshape\n",
    "# automatically assings data type\n",
    "d #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original value: 10.0\n",
      "add 1: 11.0\n",
      "subtract 5: 6.0\n"
     ]
    }
   ],
   "source": [
    "# inplace increase / decrease Variable values\n",
    "\n",
    "var.assign(10)\n",
    "print('original value:', var.numpy())\n",
    "print('add 1:', var.assign_add(1.).numpy())\n",
    "print('subtract 5:', var.assign_sub(5.).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables also have a lot of attributes associated with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name  :  my_variable:0\n",
      "type  :  <dtype: 'float32'>\n",
      "shape :  (2, 2)\n",
      "device:  /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "v = tf.Variable([[3.,3.2], [1.2,2.2]], dtype=tf.float32, name='my_variable')\n",
    "\n",
    "print('name  : ', v.name)\n",
    "print('type  : ', v.dtype)\n",
    "print('shape : ', v.shape)\n",
    "print('device: ', v.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'><b>Note</b>: Tensorflow is really similar to NumPy, and you can think of the tensors as an ndimensional array.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tf_to_np](imgs/tf_to_np.png)\n",
    "Source: CS227d, NLP, Stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Operations / Ops\n",
    "Operations can be carried out directly or assigned to variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=7>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op1 = tf.add(a,b) # nothing happens\n",
    "op1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=7>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+b # same as tf.add, here the operation is not saved in the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=189>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = a+b\n",
    "u = v+2\n",
    "w = v*u\n",
    "z = w*3\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the computational graph with @tf.function\n",
    "\n",
    "`@tf.function` is a very useful module that can be used to convert simple python functions into a highly optimized computational graph that can be run on any runtime environment. When we build a model and then train it TensorFlow we can compile the model and optimize the executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def func(a,b):\n",
    "    with tf.name_scope('first'):\n",
    "        z = tf.multiply(a,b, name='z')\n",
    "    with tf.name_scope('second'):\n",
    "        y1 = tf.constant(3, name='3')\n",
    "        y2 = tf.constant(4)\n",
    "        w1 = tf.add(z, y1, name='w1')\n",
    "        w2 = tf.add(z, y2, name='w2')\n",
    "        \n",
    "    return(w1+w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a writer to save graph information and TensorFlow logs\n",
    "# To be displayed with Tensorboard\n",
    "\n",
    "writer = tf.summary.create_file_writer(logdir) # create a writer\n",
    "tf.summary.trace_on() # trace graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(3)\n",
    "b = tf.constant(4)\n",
    "func(a,b)\n",
    "with writer.as_default():\n",
    "    tf.summary.trace_export(\n",
    "        name=\"func\",\n",
    "        step=0,\n",
    "        profiler_outdir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/tf_logs/r20190321191522/'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.14.0a20190301 at http://x1:6006 (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# run tensorboard in the shell\n",
    "!tensorboard --logdir $logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.function and Conditional statements\n",
    "It is difficult to use conditions in graphs but we could implement that easily using `@tf.function` decorator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function \n",
    "def g(x):\n",
    "    y = tf.reduce_sum(x)\n",
    "    if y > 0:\n",
    "        return y\n",
    "    return tf.abs(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\n",
      "\n",
      "def tf__g(x):\n",
      "  try:\n",
      "    with ag__.function_scope('g'):\n",
      "      do_return = False\n",
      "      retval_ = None\n",
      "      y = ag__.converted_call('reduce_sum', tf, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=ag__.Feature.ALL, internal_convert_user_code=True), (x,), {})\n",
      "      cond = ag__.gt(y, 0)\n",
      "\n",
      "      def if_true():\n",
      "        with ag__.function_scope('if_true'):\n",
      "          do_return = True\n",
      "          retval_ = y\n",
      "          return retval_\n",
      "\n",
      "      def if_false():\n",
      "        with ag__.function_scope('if_false'):\n",
      "          do_return = True\n",
      "          retval_ = ag__.converted_call('abs', tf, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=ag__.Feature.ALL, internal_convert_user_code=True), (y,), {})\n",
      "          return retval_\n",
      "      retval_ = ag__.if_stmt(cond, if_true, if_false)\n",
      "      return retval_\n",
      "  except:\n",
      "    ag__.rewrite_graph_construction_error(ag_source_map__)\n",
      "\n",
      "\n",
      "\n",
      "tf__g.autograph_info__ = {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tf.autograph.to_code(g.python_function))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient evaluation is very importnat machine learning because it is based on function optimization. You can use `tf.GradientTape()` method to record the gradient of an arbitrary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient of w^2 at 3.0 is 6.0\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(3.0)\n",
    "\n",
    "#watch the gradient of the loss operation\n",
    "with tf.GradientTape() as tape:\n",
    "    square_w = w * w\n",
    "\n",
    "grad = tape.gradient(square_w, w)\n",
    "print(f'The gradient of w^2 at {w.numpy()} is {grad.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of the Sigmoid function\n",
    "In this example we evaluate the gradient of the sigmoid function \n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "Note that \n",
    "\n",
    "$$\\sigma'(x) = \\frac{e^{-x}}{(1+e^{-x})^2} = \\sigma(x)(1-\\sigma(x)) $$\n",
    "\n",
    "For instance \n",
    "\n",
    "$$\\sigma'(0) = \\sigma(0)(1-\\sigma(0)) = \\frac{1}{2}\\left(1-\\frac{1}{2} \\right) = \\frac{1}{4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + tf.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient of the sigmoid function at 0.0 is  6.144136e-06\n"
     ]
    }
   ],
   "source": [
    "#define a varaible\n",
    "x = tf.Variable(12.)\n",
    "\n",
    "#record the gradient\n",
    "with tf.GradientTape() as tape:\n",
    "    y = sigmoid(x)\n",
    "    \n",
    "res = tape.gradient(y, x).numpy()\n",
    "print('The gradient of the sigmoid function at 0.0 is ', res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in TensorFlow\n",
    "\n",
    "This example is refactored from https://www.tensorflow.org/guide/eager. We create a complete example of using linear regression to predict the paramters of the function \n",
    "\n",
    "$$y = f(x) + noise = 3 x + 2 + noise$$\n",
    "\n",
    "Given a point $x$ we want to predict the value of $f(x)$. We train the model on 100 data pairs $(x,y)$. \n",
    "\n",
    "The model to learn is a linear model \n",
    "\n",
    "$$\\hat{y} = W x + b$$\n",
    "\n",
    "Note that, we use `tf.GradientTape` to record the gradient of the loss function with respect our model paramters.  \n",
    "\n",
    "We use MSE to calcuate the loss \n",
    "\n",
    "$$MSE = \\frac{1}{100} (y-\\hat{y})^2$$\n",
    "\n",
    "We use Gradient Descent to update the paramters \n",
    "\n",
    "$$W = W - \\alpha  \\frac{\\partial MSE}{\\partial W}$$\n",
    "\n",
    "$$b = b - \\alpha  \\frac{\\partial MSE}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f9c8c0e86a0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#100 data points \n",
    "NUM_EXAMPLES = 100\n",
    "\n",
    "#define inputs and outputs with some noise \n",
    "X = tf.random.normal([NUM_EXAMPLES])  #inputs \n",
    "noise = tf.random.normal([NUM_EXAMPLES]) #noise \n",
    "y = X * 3 + 2 + noise  #true output\n",
    "\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contruction Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model paramters with initial values \n",
    "W = tf.Variable(0.)\n",
    "b = tf.Variable(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training info\n",
    "train_steps = 300\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.trace_on() # trace graph\n",
    "\n",
    "\n",
    "#watch the gradient flow\n",
    "@tf.function  # Make it fast.\n",
    "def train_on_batch(X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #forward pass \n",
    "        yhat = X * W + b\n",
    "\n",
    "        #calcuate the loss (difference squared error)\n",
    "        error = yhat - y\n",
    "        loss = tf.reduce_mean(tf.square(error))\n",
    "\n",
    "    #evalute the gradient with the respect to the paramters\n",
    "    dW, db = tape.gradient(loss, [W, b])\n",
    "\n",
    "    #update the paramters using Gradient Descent  \n",
    "    W.assign_sub(dW * learning_rate)\n",
    "    b.assign_sub(db* learning_rate)\n",
    "\n",
    "    return(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 000: 12.866\n",
      "Loss at step 020: 6.647\n",
      "Loss at step 040: 3.682\n",
      "Loss at step 060: 2.268\n",
      "Loss at step 080: 1.594\n",
      "Loss at step 100: 1.273\n",
      "Loss at step 120: 1.120\n",
      "Loss at step 140: 1.047\n",
      "Loss at step 160: 1.012\n",
      "Loss at step 180: 0.996\n",
      "Loss at step 200: 0.988\n",
      "Loss at step 220: 0.984\n",
      "Loss at step 240: 0.982\n",
      "Loss at step 260: 0.981\n",
      "Loss at step 280: 0.981\n",
      "W : 3.096970319747925 , b  = 1.806480884552002 \n"
     ]
    }
   ],
   "source": [
    "#print the loss every 20 iterations\n",
    "for i in range(train_steps):\n",
    "    loss = train_on_batch(X,y)\n",
    "\n",
    "    \n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar('loss', loss, step=i)\n",
    "        print(\"Loss at step {:03d}: {:.3f}\".format(i, loss))\n",
    "        \n",
    "        \n",
    "print(f'W : {W.numpy()} , b  = {b.numpy()} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train_on_batch(X,y)\n",
    "with writer.as_default():\n",
    "        tf.summary.trace_export(\n",
    "        name=\"linreg\",\n",
    "        step=0,\n",
    "        profiler_outdir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.14.0a20190301 at http://x1:6006 (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir $logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9c6c7d99e8>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.plot(X, b+W*X, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Neural Network with TensorFlow and Keras\n",
    "\n",
    "These tutorials use `tf.keras`, TensorFlow's high-level Python API for building and training deep learning models. To learn more about using Keras with TensorFlow, see the TensorFlow Keras Guide.\n",
    "\n",
    "TensorFlow 2.x has integrated Keras and all it's functionality. Import all functionality from `tf.keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Keras\n",
    "Modular, powerful and intuitive Deep Learning python library built on TensorFlow, CNTK, Theano.\n",
    "* Minimalist, user-friendly interface\n",
    "* Modular\n",
    "* Deep integration with Tensorflow (`tf.keras`)\n",
    "* Works on CPUs and GPUs\n",
    "* Open-source, developed and maintained by a community of contributors, and\n",
    "publicly hosted on github\n",
    "* Extremely well documented, lots of working examples: https://keras.io/\n",
    "* Very shallow learning curve —> it is by far one of the best tools for experimenting, both for beginners and experts\n",
    "* Easy to extend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras \"Hello World\" on Iris\n",
    "\n",
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== =====\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = datasets.load_iris()\n",
    "\n",
    "print(data.DESCR[:980])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data['data']\n",
    "y = data['target']\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode y\n",
    "import pandas as pd\n",
    "\n",
    "y = pd.get_dummies(y).values\n",
    "y[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 4)\n",
      "(90, 3)\n",
      "(60, 4)\n"
     ]
    }
   ],
   "source": [
    "# train test split, plus randomize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, \n",
    "                                                    y, test_size=0.4,\n",
    "                                                    random_state=1337,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sequential model\n",
    "The simplest model in Keras is the Sequential model, a linear stack of layers. In Keras, you assemble layers to build models. A model is (usually) a graph of layers. The most common type of model is a stack of layers: the tf.keras.Sequential model.\n",
    "\n",
    "To build a simple, fully-connected network (i.e. multi-layer perceptron):\n",
    "\n",
    "* **Sequential model** Allows us to build NNs like legos, by adding one layer on top of the other, and swapping layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data structure in Keras is a model\n",
    "# The model is an object in which we organize layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model initialization\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential() # instantiate empty Sequential model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import layer classes and stack layers (in an NN model for example), by using `.add()`\n",
    "\n",
    "# Specifying the input shape\n",
    "\n",
    "The model needs to know what input shape it should expect. For this reason, the first layer in a  Sequential model needs to receive information about its input shape.\n",
    "\n",
    "**The following snippets are strictly equivalent:**\n",
    "> * `model.add(Dense(32, input_shape=(784,)))`\n",
    "> * `model.add(Dense(32, input_dim=784))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model contruction (architecture build computational graph)\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model.add( Dense(units=64, activation='relu', \\\n",
    "                 input_shape=(4,) ))\n",
    "\n",
    "model.add( Dense(units=3, activation='softmax') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilation phase, specify learning process\n",
    "\n",
    "Run `.compile()` on the model to specify learning process.\n",
    "\n",
    "Before training a model, you need to configure the learning process, which is done via the  compile method. It receives three arguments:\n",
    "\n",
    "* **A loss function:** This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as `categorical_crossentropy` or `mse`), or it can be an objective function.\n",
    "* **An optimizer:** This could be the string identifier of an existing optimizer (such as `rmsprop`, `gradientdescent`, or `adam`), or an instance of the Optimizer class.\n",
    "* **(Optional) A list of metrics:** For any classification problem you will want to set this to `metrics=['accuracy']`. A metric could be the string identifier of an existing metric or a custom metric function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can also specify our own optimizer or loss function (even build it ourselves)\n",
    "\n",
    "```python\n",
    "# or with we can specify loss function or optimizer\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = SGD(lr=0.001, momentum = 0.9, nesterov=True),\n",
    "             metrics = ['accuracy'])\n",
    "```\n",
    "\n",
    "### Different optimizers and their trade-offs\n",
    "To read more about gradient descent optimizers, hyperparameters etc. This is a recommended reading: http://ruder.io/optimizing-gradient-descent/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.1061 - accuracy: 0.3778\n",
      "Epoch 2/50\n",
      "90/90 [==============================] - 0s 58us/sample - loss: 1.0982 - accuracy: 0.3778\n",
      "Epoch 3/50\n",
      "90/90 [==============================] - 0s 118us/sample - loss: 1.0912 - accuracy: 0.3778\n",
      "Epoch 4/50\n",
      "90/90 [==============================] - 0s 69us/sample - loss: 1.0849 - accuracy: 0.3778\n",
      "Epoch 5/50\n",
      "90/90 [==============================] - 0s 104us/sample - loss: 1.0790 - accuracy: 0.3778\n",
      "Epoch 6/50\n",
      "90/90 [==============================] - 0s 95us/sample - loss: 1.0727 - accuracy: 0.3778\n",
      "Epoch 7/50\n",
      "90/90 [==============================] - 0s 110us/sample - loss: 1.0662 - accuracy: 0.3778\n",
      "Epoch 8/50\n",
      "90/90 [==============================] - 0s 77us/sample - loss: 1.0588 - accuracy: 0.3778\n",
      "Epoch 9/50\n",
      "90/90 [==============================] - 0s 123us/sample - loss: 1.0518 - accuracy: 0.3778\n",
      "Epoch 10/50\n",
      "90/90 [==============================] - 0s 114us/sample - loss: 1.0444 - accuracy: 0.3778\n",
      "Epoch 11/50\n",
      "90/90 [==============================] - 0s 93us/sample - loss: 1.0363 - accuracy: 0.3778\n",
      "Epoch 12/50\n",
      "90/90 [==============================] - 0s 83us/sample - loss: 1.0275 - accuracy: 0.3778\n",
      "Epoch 13/50\n",
      "90/90 [==============================] - 0s 112us/sample - loss: 1.0179 - accuracy: 0.4111\n",
      "Epoch 14/50\n",
      "90/90 [==============================] - 0s 87us/sample - loss: 1.0072 - accuracy: 0.6444\n",
      "Epoch 15/50\n",
      "90/90 [==============================] - 0s 80us/sample - loss: 0.9955 - accuracy: 0.6778\n",
      "Epoch 16/50\n",
      "90/90 [==============================] - 0s 86us/sample - loss: 0.9836 - accuracy: 0.6333\n",
      "Epoch 17/50\n",
      "90/90 [==============================] - 0s 68us/sample - loss: 0.9711 - accuracy: 0.5667\n",
      "Epoch 18/50\n",
      "90/90 [==============================] - 0s 79us/sample - loss: 0.9585 - accuracy: 0.6556\n",
      "Epoch 19/50\n",
      "90/90 [==============================] - 0s 64us/sample - loss: 0.9453 - accuracy: 0.7000\n",
      "Epoch 20/50\n",
      "90/90 [==============================] - 0s 79us/sample - loss: 0.9326 - accuracy: 0.6889\n",
      "Epoch 21/50\n",
      "90/90 [==============================] - 0s 85us/sample - loss: 0.9190 - accuracy: 0.6778\n",
      "Epoch 22/50\n",
      "90/90 [==============================] - 0s 103us/sample - loss: 0.9046 - accuracy: 0.6556\n",
      "Epoch 23/50\n",
      "90/90 [==============================] - 0s 64us/sample - loss: 0.8908 - accuracy: 0.6556\n",
      "Epoch 24/50\n",
      "90/90 [==============================] - 0s 95us/sample - loss: 0.8778 - accuracy: 0.6556\n",
      "Epoch 25/50\n",
      "90/90 [==============================] - 0s 63us/sample - loss: 0.8630 - accuracy: 0.6556\n",
      "Epoch 26/50\n",
      "90/90 [==============================] - 0s 71us/sample - loss: 0.8494 - accuracy: 0.6556\n",
      "Epoch 27/50\n",
      "90/90 [==============================] - 0s 70us/sample - loss: 0.8359 - accuracy: 0.6556\n",
      "Epoch 28/50\n",
      "90/90 [==============================] - 0s 57us/sample - loss: 0.8224 - accuracy: 0.6556\n",
      "Epoch 29/50\n",
      "90/90 [==============================] - 0s 65us/sample - loss: 0.8097 - accuracy: 0.6556\n",
      "Epoch 30/50\n",
      "90/90 [==============================] - 0s 72us/sample - loss: 0.7960 - accuracy: 0.6556\n",
      "Epoch 31/50\n",
      "90/90 [==============================] - 0s 95us/sample - loss: 0.7832 - accuracy: 0.6556\n",
      "Epoch 32/50\n",
      "90/90 [==============================] - 0s 95us/sample - loss: 0.7708 - accuracy: 0.6667\n",
      "Epoch 33/50\n",
      "90/90 [==============================] - 0s 123us/sample - loss: 0.7592 - accuracy: 0.7000\n",
      "Epoch 34/50\n",
      "90/90 [==============================] - 0s 61us/sample - loss: 0.7469 - accuracy: 0.7000\n",
      "Epoch 35/50\n",
      "90/90 [==============================] - 0s 101us/sample - loss: 0.7354 - accuracy: 0.7111\n",
      "Epoch 36/50\n",
      "90/90 [==============================] - 0s 59us/sample - loss: 0.7243 - accuracy: 0.7111\n",
      "Epoch 37/50\n",
      "90/90 [==============================] - 0s 105us/sample - loss: 0.7133 - accuracy: 0.7111\n",
      "Epoch 38/50\n",
      "90/90 [==============================] - 0s 74us/sample - loss: 0.7031 - accuracy: 0.7333\n",
      "Epoch 39/50\n",
      "90/90 [==============================] - 0s 85us/sample - loss: 0.6932 - accuracy: 0.7222\n",
      "Epoch 40/50\n",
      "90/90 [==============================] - 0s 64us/sample - loss: 0.6832 - accuracy: 0.7333\n",
      "Epoch 41/50\n",
      "90/90 [==============================] - 0s 71us/sample - loss: 0.6740 - accuracy: 0.7889\n",
      "Epoch 42/50\n",
      "90/90 [==============================] - 0s 124us/sample - loss: 0.6656 - accuracy: 0.8000\n",
      "Epoch 43/50\n",
      "90/90 [==============================] - 0s 74us/sample - loss: 0.6569 - accuracy: 0.8222\n",
      "Epoch 44/50\n",
      "90/90 [==============================] - 0s 80us/sample - loss: 0.6487 - accuracy: 0.8111\n",
      "Epoch 45/50\n",
      "90/90 [==============================] - 0s 70us/sample - loss: 0.6410 - accuracy: 0.8222\n",
      "Epoch 46/50\n",
      "90/90 [==============================] - 0s 90us/sample - loss: 0.6332 - accuracy: 0.8222\n",
      "Epoch 47/50\n",
      "90/90 [==============================] - 0s 91us/sample - loss: 0.6263 - accuracy: 0.8222\n",
      "Epoch 48/50\n",
      "90/90 [==============================] - 0s 70us/sample - loss: 0.6192 - accuracy: 0.8222\n",
      "Epoch 49/50\n",
      "90/90 [==============================] - 0s 61us/sample - loss: 0.6122 - accuracy: 0.8222\n",
      "Epoch 50/50\n",
      "90/90 [==============================] - 0s 96us/sample - loss: 0.6058 - accuracy: 0.8222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c8f264a58>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model by iterating over the training data in batches\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 50, batch_size= 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666667"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Evaluate the model Accuracy on test set\n",
    "model.evaluate(X_test, y_test,verbose=False)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on new data:\n",
    "\n",
    "class_probabilities = model.predict(X_test)\n",
    "\n",
    "# gives output of the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10899376, 0.45341527, 0.43759093],\n",
       "       [0.77494735, 0.16220754, 0.06284513],\n",
       "       [0.05490121, 0.41457802, 0.5305208 ],\n",
       "       [0.7710714 , 0.16476808, 0.06416058],\n",
       "       [0.05371424, 0.41313815, 0.53314763]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_probabilities[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST: Intro to NN in TensorFlow\n",
    "\n",
    "Example taken from Google Docs\n",
    "\n",
    "We are now going to recognize hand-written digits.\n",
    "\n",
    "![https://www.tensorflow.org/images/MNIST.png](https://www.tensorflow.org/images/MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the most classic NN dataset\n",
    "The MNIST data is split into three parts: 60,000 data points of training data (mnist.train), 10,000 points of test data (mnist.test).\n",
    "\n",
    "Every MNIST data point has two parts: an image of a handwritten digit and a corresponding label. We'll call the images \"x\" and the labels \"y\". Both the training set and test set contain images and their corresponding labels; for example the training images are mnist.train.images and the training labels are mnist.train.labels.\n",
    "\n",
    "Each image is 28 pixels by 28 pixels. We can interpret this as a big array of numbers:\n",
    "\n",
    "![https://www.tensorflow.org/images/MNIST-Matrix.png](https://www.tensorflow.org/images/MNIST-Matrix.png)\n",
    "\n",
    "We can flatten this array into a vector of 28x28 = 784 numbers. It doesn't matter how we flatten the array, as long as we're consistent between images. From this perspective, the MNIST images are just a bunch of points in a 784-dimensional vector space, with a very rich structure (warning: computationally intensive visualizations).\n",
    "\n",
    "Flattening the data throws away information about the 2D structure of the image. Isn't that bad? Well, the best computer vision methods do exploit this structure, and we will in later tutorials. But the simple method we will be using here, a softmax regression (defined below), won't.\n",
    "\n",
    "The result is that mnist.train.images is a tensor (an n-dimensional array) with a shape of [60000, 784]. The first dimension is an index into the list of images and the second dimension is the index for each pixel in each image. Each entry in the tensor is a pixel intensity between 0 and 1, for a particular pixel in a particular image.\n",
    "\n",
    "![https://www.tensorflow.org/images/mnist-train-xs.png](https://www.tensorflow.org/images/mnist-train-xs.png)\n",
    "\n",
    "Each image in MNIST has a corresponding label, a number between 0 and 9 representing the digit drawn in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/MNIST_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras. \\\n",
    "                            datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input shape: (60000, 28, 28)\n",
      "Test input shape: (10000, 28, 28)\n",
      "Input data type: uint8\n"
     ]
    }
   ],
   "source": [
    "# input information\n",
    "print('Train input shape:',x_train.shape)\n",
    "print('Test input shape:',x_test.shape)\n",
    "print('Input data type:',x_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-max values: 0 255\n"
     ]
    }
   ],
   "source": [
    "print('Min-max values:',np.min(x_train),np.max(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Image:')\n",
    "plt.imshow(x_train[1],cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train output shape: (60000,)\n",
      "Test output shape: (10000,)\n",
      "Data type: uint8\n"
     ]
    }
   ],
   "source": [
    "# Output information:\n",
    "print('Train output shape:',y_train.shape)\n",
    "print('Test output shape:',y_test.shape)\n",
    "print('Data type:',y_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [0 1 2 3 4 5 6 7 8 9]\n",
      "First 10 outputs:\n",
      "[5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "print('Unique labels:',np.unique(y_train))\n",
    "print('First 10 outputs:')\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data, flatten inputs, and convert datatype\n",
    "x_train = x_train.reshape(60000, 784). \\\n",
    "                    astype('float32') / 255\n",
    "\n",
    "x_test = x_test.reshape(10000, 784) \\\n",
    "                    .astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load standard NN components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model constructor\n",
    "model = Sequential()\n",
    "# Add layers sequentially\n",
    "model.add(Dense(300, activation=tf.nn.relu, \\\n",
    "                    input_shape=(784,)))\n",
    "\n",
    "# Second..\n",
    "model.add(Dense(200, activation=tf.nn.relu))\n",
    "\n",
    "# Third..\n",
    "model.add(Dense(100, activation=tf.nn.relu))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "750/750 [==============================] - 8s 10ms/step - loss: 0.2474 - accuracy: 0.9246 - val_loss: 0.1191 - val_accuracy: 0.9626\n",
      "Epoch 2/5\n",
      "750/750 [==============================] - 8s 11ms/step - loss: 0.1023 - accuracy: 0.9694 - val_loss: 0.1024 - val_accuracy: 0.9714\n",
      "Epoch 3/5\n",
      "750/750 [==============================] - 8s 11ms/step - loss: 0.0680 - accuracy: 0.9803 - val_loss: 0.1004 - val_accuracy: 0.9736\n",
      "Epoch 4/5\n",
      "750/750 [==============================] - 9s 12ms/step - loss: 0.0523 - accuracy: 0.9844 - val_loss: 0.1022 - val_accuracy: 0.9772\n",
      "Epoch 5/5\n",
      "750/750 [==============================] - 8s 10ms/step - loss: 0.0405 - accuracy: 0.9881 - val_loss: 0.0953 - val_accuracy: 0.9772\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "NO_EPOCHS = 5\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=NO_EPOCHS,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9810000061988831\n"
     ]
    }
   ],
   "source": [
    "test_scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "#print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot accuracy\n",
    "plt.plot(range(NO_EPOCHS),history.history['val_accuracy']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# device placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place tensors on the CPU\n",
    "with tf.device('/cpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.data.DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(x, y):\n",
    "    #normalize and expand\n",
    "    x = tf.cast(x, tf.float32)/255.\n",
    "    x = tf.expand_dims(x, -1)\n",
    "\n",
    "    #cast the labels\n",
    "    y = tf.cast(y, tf.int32)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset(x, y):  \n",
    "    #convert to tensors and shuffle\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x , y)).shuffle(len(x)-1)\n",
    "\n",
    "    #extract batches\n",
    "    dataset = dataset.batch(32)\n",
    "\n",
    "    #preprocess the batch\n",
    "    dataset = dataset.map(pre_process, num_parallel_calls = 4)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data (flatten input, convert to floating point and normalize)\n",
    "# Then convert to Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype('float32') / 255, y_train))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "dataset = dataset.shuffle(buffer_size=60000)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batches\n",
    "dataset = dataset.batch(64)\n",
    "dataset # creates a new dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Vanilla) ANN Network structure\n",
    "* Any Neural Network with one hidden layer can be a Universal Function Approximator. Source: [https://en.wikipedia.org/wiki/Universal_approximation_theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)\n",
    "* The number of input nodes are equal to the number of features\n",
    "* The number of output nodes are equal to the number of classes (for classification tasks)\n",
    "* A bias term is added to every layer that only feeds in a 1, that adds an extra degree of freedom for every functional input value to the next function\n",
    "\n",
    "# How deep should we go?\n",
    "* We can overfit Neural Nets, one way to combat that is by using dropout and regularization\n",
    "* Predictions will usually be better when we increase depth of network and widen it (increase the number of neurons in every layer)\n",
    "\n",
    "# Activation Functions\n",
    "* Classically the sigmoid function was used in the hidden layers (simplest function between 0 - 1). Logit function.\n",
    "* Nowadays it is more common to use the ReLU (Rectified Linear Unit). Much quicker! For deep networks sigmoid might not want to converge at all. Much better to handle exploding and vanishing gradients (Leaky Relu). Can also combat that with *Batch Normalization*.\n",
    "* For the input layer we send in the (standardized) values.\n",
    "* For the output layer we often use a softmax function (multi-class classification) or a sigmoid function (binary classification). Softmax only works if the classes are mutually exclusive, i.e. we only try to label one pattern in every training example.\n",
    "\n",
    "\n",
    "# Training algorithm steps\n",
    "* Train a model to make a prediction\n",
    "* Compute distance between predictions and true values\n",
    "* Modify weights and biases to lower error\n",
    "\n",
    "\n",
    "# Overfitting\n",
    "* Mostly because our network has too many degrees of freedom (neurons in the network)\n",
    "* Can use L1 and L2 regularization on the cost function\n",
    "* Drop out (used to mitigate the effects of too many degrees of freedom)\n",
    "\n",
    "# ANNs are not great at classifying images\n",
    "* We don't make use of the image shapes and curves. Shape info is lost when we flatten arrays.\n",
    "\n",
    "# ANN One Layer Softmax Classification\n",
    "\n",
    "What we will accomplish in this section:\n",
    "\n",
    "- Create a softmax regression function that is a model for recognizing MNIST digits, based on looking at every pixel in the image\n",
    "- Use Tensorflow to train the model to recognize digits by having it \"look\" at thousands of examples (and run our first Tensorflow session to do so)\n",
    "- Check the model's accuracy with our test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced tf2 model training\n",
    "\n",
    "Work in progress (porting to tf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters and input size\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neuron layers (ReLU in hidden layers)\n",
    "# We'll take care of Softmax for output with loss function\n",
    "\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    # X input to neuron\n",
    "    # number of neurons for the layer\n",
    "    # name of layer\n",
    "    # pass in eventual activation function\n",
    "    n_inputs = int(X.shape[1])\n",
    "\n",
    "    # initialize weights to prevent vanishing / exploding gradients\n",
    "    stddev = 2 / np.sqrt(n_inputs)\n",
    "    init = tf.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "    # Initialize weights for the layer\n",
    "    W = tf.Variable(init((n_inputs, n_neurons)), name=\"weights\")\n",
    "    # biases\n",
    "    b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "\n",
    "    # Output from every neuron\n",
    "    Z = tf.matmul(X, W) + b\n",
    "    if activation is not None:\n",
    "        return activation(Z)\n",
    "    else:\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = neuron_layer(x_train, n_hidden1, name=\"hidden1\",\n",
    "                       activation=tf.nn.relu)\n",
    "hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                       activation=tf.nn.relu)\n",
    "logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step with Gradient Descent\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function (that also optimizes Softmax for output):\n",
    "\n",
    "\n",
    "# logits are from the last output of the dnn\n",
    "xentropy = tf.keras.losses.sparse_categorical_crossentropy(y_train, logits, from_logits=True)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def train_step(x, label):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         predictions = model(x)\n",
    "#         loss = loss_object(label, predictions)\n",
    "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "# \n",
    "#     train_loss(loss)\n",
    "#     train_accuracy(label, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
